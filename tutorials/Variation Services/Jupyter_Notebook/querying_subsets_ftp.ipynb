{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieving subsets of our data from NCBI's FTP site\n",
        "\n",
        "In this notebook we show how to retrieve subsets from our data in VCF format from NCBI's FTP site. Our VCF content is compressed and has a `tabix` index.\n",
        "\n",
        "`pytabix` takes advantage of the partial downloads available via `tabix` indexing over FTP. You can also use the `tabix` program directly from the command-line. In this notebook we will use `pytabix` to perform queries because many people prefer to do their analysis in Python.\n",
        "\n",
        "*Note*: You cannot run this notebook at [Binder](https://mybinder.org/) because they [do not allow](https://github.com/binder-examples/getting-data#large-public-files) FTP connections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --quiet \"urllib3<2\" pytabix pysam ratelimit requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetching a range of variants\n",
        "\n",
        "Using the code below we first tell `tabix` where to find the compressed VCF file, and then use the method `query` to retrieve the single variant at `905373` on chromosome 1. This requires starting the query at `905372` and ending on `905373`. Larger regions are also possible.\n",
        "\n",
        "This is really all that is needed to get the basic data. Everything else is for display and formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tabix\n",
        "\n",
        "ftp_server='ftp://ftp.ncbi.nlm.nih.gov'\n",
        "ftp_path='/snp/population_frequency/latest_release/'\n",
        "ftp_vcf_path=ftp_path + 'freq.vcf.gz'\n",
        "ftp_idx_path=ftp_path + 'freq.vcf.gz.tbi'\n",
        "tb = tabix.open(ftp_server + ftp_vcf_path, ftp_server + ftp_idx_path)\n",
        "\n",
        "records = tb.query('NC_000001.11', 905372, 905373)\n",
        "\n",
        "for row in records:\n",
        "    print(str(row))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fetching the header and metadata\n",
        "\n",
        "As mentioned earlier, `pytabix` does not have the ability to retrieve the VCF headers. We need to parse the headers separately. Given the size of our FTP files, it would be too time and space consuming to do that. Instead, we will show you how to download a portion of the file that will allow us to parse the headers, but that is small enough that it only takes a very short time to obtain it.\n",
        "\n",
        "To do this we use a small function `fetch_bytes` that retrieves just the first `N` kilobytes of data (set in the `MAX_BYTES` variable) of the compressed file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ftplib import FTP\n",
        "from io import BytesIO\n",
        "\n",
        "MAX_BYTES = 1024 * 8\n",
        "\n",
        "\n",
        "def fetch_bytes(ftp: FTP, ftp_file: str, max_bytes: int = MAX_BYTES) -> BytesIO:\n",
        "    \"\"\"\n",
        "    Fetches at most max_bytes from the FTP file passed as input\n",
        "\n",
        "    Adapted from https://stackoverflow.com/a/53144697\n",
        "    \"\"\"\n",
        "    ftp_data = BytesIO()\n",
        "\n",
        "    def data_size(biost: BytesIO):\n",
        "        return biost.getbuffer().nbytes\n",
        "\n",
        "    size = ftp.size(ftp_file)\n",
        "\n",
        "    ftp.voidcmd('TYPE I')\n",
        "    conn = ftp.transfercmd(f\"RETR {ftp_file}\", 0)\n",
        "\n",
        "    try:\n",
        "        while data_size(ftp_data) < max_bytes:\n",
        "            buf = conn.recv(\n",
        "                min(size - data_size(ftp_data), max_bytes))\n",
        "            if not buf:\n",
        "                break\n",
        "            ftp_data.write(buf)\n",
        "    finally:\n",
        "        conn.close()\n",
        "    try:\n",
        "        ftp.voidresp()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    ftp_data.seek(0)\n",
        "    return ftp_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can call `fetch_bytes` to retrieve enough data from our compressed VCF file to make sure we have the VCF header locally, which is all we need. The function puts all those bytes into a `BytesIO` object. We can then parse the object using gzip. \n",
        "\n",
        "Finally, we print the data obtained with `tabix` just below the headers so that it is clear what data correspons to what fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ftplib import FTP\n",
        "\n",
        "import gzip\n",
        "from sys import stdout\n",
        "\n",
        "ftp = FTP('ftp.ncbi.nlm.nih.gov')\n",
        "ftp.login()\n",
        "\n",
        "ftpfile = fetch_bytes(ftp, ftp_vcf_path)\n",
        "\n",
        "f = gzip.open(ftpfile,'rb')\n",
        "\n",
        "# Now print the full headers\n",
        "for l in f:\n",
        "    l = l.decode().rstrip()\n",
        "    if '##' in l:\n",
        "        print(l)\n",
        "    elif '#CHROM' in l:\n",
        "        samples = l.split()[9:]\n",
        "        print(l)\n",
        "    else:\n",
        "        break\n",
        "f.close()\n",
        "\n",
        "# Now we print the rows that `tabix` found based on our query\n",
        "records = tb.query('NC_000001.11', 905372, 905373)\n",
        "for row in records:\n",
        "    print('\\t'.join(row))\n",
        "    \n",
        "ftp.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can observe that the `FORMAT` used is `AN:AC`. The meaning of each of those two acronyms is shown in the header:\n",
        "\n",
        "* `AN`: _Total allele count for the population, including `REF`_\n",
        "* `AC`: _Allele count for each `ALT` allele for the population_\n",
        "\n",
        "The accessions, or biosample ids, of those populations are shown starting on the tenth column of the fifth line. Their frequency data in `AN:AC` format is shown under those headers starting on the 6th line.\n",
        "\n",
        "You can check the names of the populations in the link provided in the header:\n",
        "\n",
        "https://www.ncbi.nlm.nih.gov/biosample/?term=popfreq\n",
        "\n",
        "In the remainder of this tutorial we show how to programmatically print those frequencies with the population names instead of their accessions. To achieve this, we first show how to retrieve population metadata and create a dictionary from biosample ids to population names. Then we use that dictionary and the code in this section to print the frequencies in an easier to understand format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieving and using population metadata\n",
        "\n",
        "In other notebooks we presented code to retrieve study and population metadata from our services and to index the populations by their biosample ids. For convenience, we repeat some of that code here, and also add a convenience function to map biosample ids to population names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from ratelimit import limits\n",
        "from requests import get, codes as http_code\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "\n",
        "VarFreqMetadata = List[Dict[str, Any]]\n",
        "MetadataDict = Dict[str, Dict[str, Any]]\n",
        "\n",
        "\n",
        "@limits(calls=1, period=1)  # Only one call per second\n",
        "def get_metadata() -> VarFreqMetadata:\n",
        "    \"\"\"\n",
        "    Retrieve information that describes all studies and populations\n",
        "    used by the frequency endpoints\n",
        "    \"\"\"\n",
        "    METADATA_URL = (\"https://api.ncbi.nlm.nih.gov/variation/v0/\"\n",
        "                    \"metadata/frequency\")\n",
        "\n",
        "    reply = get(METADATA_URL)\n",
        "    if reply.status_code != http_code.ok:\n",
        "        raise Exception(\"Request failed: {}\\n{}\".format(\n",
        "            reply.status_code, METADATA_URL))\n",
        "\n",
        "    content_type = reply.headers['content-type']\n",
        "    if content_type != 'application/json':\n",
        "        raise Exception(\"Unexpected content type: {}\\n{}\".format(\n",
        "            content_type, METADATA_URL))\n",
        "\n",
        "    return reply.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_accession_to_name_dict(pop_list: List[MetadataDict]) -> MetadataDict:\n",
        "    \"\"\"\n",
        "    Constructs a dictionary of biosample ids to population names for each\n",
        "    population (biosample) in the list of populations received as input.\n",
        "    \"\"\"\n",
        "    result: MetadataDict = dict()\n",
        "    for population in pop_list:\n",
        "        result.update({population[\"biosample_id\"]: population[\"name\"]})\n",
        "\n",
        "        if 'subs' in population:\n",
        "            subs_dict = build_accession_to_name_dict(\n",
        "                    population['subs'])\n",
        "            if subs_dict:\n",
        "                result.update(subs_dict)\n",
        "    return result\n",
        "\n",
        "def build_population_dictionary(metadata_orig: VarFreqMetadata) -> MetadataDict:\n",
        "    \"\"\"\n",
        "    Constructs a dictionary of biosample ids to population names for each\n",
        "    study (bioproject) listed in the frequency metadata received as input.\n",
        "    \"\"\"\n",
        "    result: MetadataDict = dict()\n",
        "\n",
        "    for study in metadata_orig:\n",
        "        result.update(\n",
        "            build_accession_to_name_dict(study[\"populations\"]))\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can use these functions to first retrieve the population metadata and then to create a map from biosample ids to population names. Finally, we print the name of the population with accession `SAMN10492705` as an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metadata = get_metadata()\n",
        "pop_dict = build_population_dictionary(metadata)\n",
        "\n",
        "print(pop_dict[\"SAMN10492705\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Displaying frequency data in a more user-friendly manner\n",
        "\n",
        "We are finally ready to print the data in the VCF file.\n",
        "\n",
        "First we translate the biosamples in the VCF file to their names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print header with population names\n",
        "for biosample_id in samples:\n",
        "    print(biosample_id, ': ', pop_dict[biosample_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we print the subset of data in the VCF file that we queried before in a more user-friendly manner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# same query as before\n",
        "data = tb.query('NC_000001.11', 905372, 905373)\n",
        "\n",
        "for row_num, row in enumerate(data):\n",
        "    print('Row: ', row_num)\n",
        "    for column, biosample_id in enumerate(samples):\n",
        "        print('\\t', pop_dict[biosample_id])\n",
        "        counts = row[9:][column]\n",
        "        an, ac = counts.split(':')\n",
        "        print('\\t\\t', 'Total allele count (including REF):', an)\n",
        "        print('\\t\\t', 'Allele count for each ALT allele for this population:', ac)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}